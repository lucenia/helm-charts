# clusterName is the name used for the lucenia cluster; this should be unique per namespace
clusterName: "lucenia-cluster"
# nodeGroup is the name used for each group of nodes in the cluster
nodeGroup: "manager"

# managerService is the service that non-manager groups will try to connect to when joining the cluster
# This should be set to clusterName + "-" + nodeGroup for your manager group.
managerService: ""

# singleNode is a boolean that determines if the cluster will be created with 1 replica
# Set to true if Lucenia cluster config discovery.type is set to single-node
# If true, the cluster will be created with 1 replica by default
# If false, the cluster will be created the replicaCount number of nodes
singleNode: false
# replicaCount is the number of nodes in the Lucenia cluster
replicaCount: 3

# roles represent the Lucenia node roles settings applied to nodeGroup
# These will be set as an environment variable "node.roles".
# For example, node.roles=cluster_manager,ingest,data,remote_cluster_client
roles:
  - cluster_manager
  - ingest
  - data
  - remote_cluster_client

# majorVersion is the major version of Lucenia to deploy; set this when using a custom image
# If not set, majorVersion defaults to .Values.imageTag, then .Chart.AppVersion
majorVersion: ""

global:
  # dockerRegistry is used to change the default docker registry; for example, a private registry
  dockerRegistry: ""

# luceniaHome defines the home directory for the Lucenia cluster
# Allows config files to be added to {{ .Values.luceniaHome }}/config
luceniaHome: "/usr/share/lucenia"

# config is used to add configuration such as `lucenia.yml` and `log4j2.properties`
config:
  # Values must be YAML literal style scalar / YAML multiline string.
  # <filename>: |
  #   <formatted-value(s)>
  # log4j2.properties: |
  #   status = error
  #
  #   appender.console.type = Console
  #   appender.console.name = console
  #   appender.console.layout.type = PatternLayout
  #   appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n
  #
  #   rootLogger.level = info
  #   rootLogger.appenderRef.console.ref = console
  node.pem: |
    placeholder: create demo cert file
  node-key.pem: |
    placeholder: create demo cert file
  root-ca.pem: |
    placeholder: create demo cert file
  lucenia.yml: |
    cluster.name: lucenia-cluster

    # Bind to all interfaces because we don't know what IP address Docker will assign to us.
    network.host: 0.0.0.0

    # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
    # Implicitly done if ".singleNode" is set to "true".
    # discovery.type: single-node

    # Start Lucenia Security Demo Configuration
    # WARNING: revise all the lines below before you go into production
    plugins:
      license:
        certificate_filepath: license/license.crt
      security:
        ssl:
          transport:
            pemcert_filepath: node.pem
            pemkey_filepath: node-key.pem
            pemtrustedcas_filepath: root-ca.pem
            enforce_hostname_verification: false
          http:
            enabled: true
            pemcert_filepath: node.pem
            pemkey_filepath: node-key.pem
            pemtrustedcas_filepath: root-ca.pem
        allow_unsafe_democertificates: true
        allow_default_init_securityindex: true
        authcz:
          admin_dn:
            - CN=kirk,OU=client,O=client,L=test,C=de
        audit.type: internal_lucenia
        enable_snapshot_restore_privilege: true
        check_snapshot_restore_write_privileges: true
        restapi:
          roles_enabled: ["all_access", "security_rest_api_access"]
        system_indices:
          enabled: false
          indices:
            [
              # define list of indices that are managed by security
            ]
    ######## End Lucenia Security Demo Configuration ########
  # log4j2.properties:

# extraEnvs are additional environment variables to set for  the nodeGroup
# Will be appended to the current 'env:' key. You can use any kubernetes env syntax
extraEnvs: []
#  - name: MY_ENVIRONMENT_VAR
#    value: the_value_goes_here
# A custom strong password must be set in order to setup a demo admin user.
# Lucenia cluster will not spin-up without this unless demo config install is disabled.
#  - name: LUCENIA_INITIAL_ADMIN_PASSWORD
#    value: <strong-password>

# envFrom is used to load environment variables from Kubernetes secrets or config maps
envFrom: []
# - secretRef:
#     name: env-secret
# - configMapRef:
#     name: config-map

# secretMounts is a list of secrets and paths to mount inside the pod
# For example, mounting certificates for security
secretMounts: []

# hostAliases is used to add entries to the pod's /etc/hosts file
hostAliases: []
# - ip: "127.0.0.1"
#   hostnames:
#   - "foo.local"
#   - "bar.local"

# image is the Lucenia Docker image
image:
  # repository is the image repository and name
  repository: "lucenia/lucenia"
  # pullPolicy is the Kubernetes imagePullPolicy
  pullPolicy: IfNotPresent
  # tag overrides the image tag whose default is the chart appVersion.
  tag: ""

# imagePullSecrets is used to set secrets to pull images from a private registry
imagePullSecrets: []

# nameOverride is used to override the `clusterName` when naming resources
nameOverride: ""

# fullnameOverride is used to override the `clusterName` and `nodeGroup` when naming resources
# This should only be used when using a single `nodeGroup`, otherwise there will be name conflicts
fullnameOverride: ""

# podAnnotations is used to add annotations to all Lucenia pods
podAnnotations: {}
#   "key": "value"

# luceniaAnnotations adds annotations to the Lucenia Statefulset
luceniaAnnotations: {}

# labels is used to add labels to all Lucenia pods
labels: {}

# luceniaJavaOpts sets the Java options for Lucenia
luceniaJavaOpts: "-Xmx512M -Xms512M"

# resources is used to set the resources for the Lucenia StatefulSet
resources:
  requests:
    cpu: "1000m"
    memory: "100Mi"

# initResources is used to set resources for the init container in the StatefulSet
initResources: {}
#  limits:
#     cpu: "25m"
#     memory: "128Mi"
#  requests:
#     cpu: "25m"
#     memory: "128Mi"

# sidecarResources is used to set resources for the sidecar container in the StatefulSet
sidecarResources: {}
#   limits:
#     cpu: "25m"
#     memory: "128Mi"
#   requests:
#     cpu: "25m"
#     memory: "128Mi"

# networkHost is used as the value for network.host in the Lucenia configuration
networkHost: "0.0.0.0"

# rbac is used to specify settings for RBAC resources - role, rolebinding, serviceaccount
rbac:
  # create is a boolean that specifies whether RBAC resources should be created
  create: false
  # serviceAccountAnnotations are annotations to add to the ServiceAccount
  serviceAccountAnnotations: {}
  # serviceAccountName is used to override the name of the ServiceAccount created by the chart
  serviceAccountName: ""
  # automountServiceAccountToken controls whether the Service Account token is automatically mounted to the Lucenia StatefulSet pods
  automountServiceAccountToken: false

# podSecurityPolicy is used to specify settings for the PodSecurityPolicy
podSecurityPolicy:
  create: false
  name: ""
  spec:
    privileged: true
    fsGroup:
      rule: RunAsAny
    runAsUser:
      rule: RunAsAny
    seLinux:
      rule: RunAsAny
    supplementalGroups:
      rule: RunAsAny
    volumes:
      - secret
      - configMap
      - persistentVolumeClaim
      - emptyDir

# persistence is used to specify settings for Lucenia persistent volumes
persistence:
  enabled: true
  # Set to false to disable the `fsgroup-volume` initContainer that will update permissions on the persistent disk.
  enableInitChown: true
  # override image, which is busybox by default
  # image: busybox
  # override image tag, which is latest by default
  # imageTag:
  labels:
    # Add default labels for the volumeClaimTemplate of the StatefulSet
    enabled: false
  # Lucenia Persistent Volume Storage Class
  # If storageClass is defined, storageClassName: <storageClass>
  # If storageClass is set to "-", storageClassName: "", which disables dynamic provisioning
  # If undefined (the default) or set to null, no storageClassName spec is set,
  #   choosing the default provisioner.

  # storageClass: "-"
  accessModes:
    - ReadWriteOnce
  size: 8Gi
  annotations: {}

# extraVolumes is used to add extra volumes to the Lucenia StatefulSet
extraVolumes: []
#   - name: extras
#     emptyDir: {}

# extraVolumeMounts is used to add extra volume mounts to the Lucenia StatefulSet
extraVolumeMounts: []
#   - name: extras
#     mountPath: /usr/share/extras
#     readOnly: true

# extraContainers is used to add containers to the Lucenia StatefulSet
extraContainers: []
#   - name: do-something
#     image: busybox
#     command: ['do', 'something']

# extraInitContainers is used to add init containers to the Lucenia StatefulSet
extraInitContainers: []
#   - name: do-somethings
#     image: busybox
#     command: ['do', 'something']

# This is the PriorityClass settings as defined in
# https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
priorityClassName: ""

# By default this will make sure two pods don't end up on the same node
# Changing this to a region would allow you to spread pods across regions
antiAffinityTopologyKey: "kubernetes.io/hostname"

# Hard means that by default pods will only be scheduled if there are enough nodes for them
# and that they will never end up on the same node. Setting this to soft will do this "best effort".
# Setting this to custom will use what is passed into customAntiAffinity.
antiAffinity: "soft"

# Allows passing in custom anti-affinity settings as defined in
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#types-of-inter-pod-affinity-and-anti-affinity
# Using this parameter requires setting antiAffinity to custom.
customAntiAffinity: {}

# This is the node affinity settings as defined in
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
nodeAffinity: {}

# This is the pod affinity settings as defined in
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#types-of-inter-pod-affinity-and-anti-affinity
podAffinity: {}

# This is the pod topology spread constraints
# https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
topologySpreadConstraints: []

# The default is to deploy all pods serially. By setting this to parallel all pods are started at
# the same time when bootstrapping the cluster
podManagementPolicy: "Parallel"

# The environment variables injected by service links are not used, but can lead to slow Lucenia boot times when
# there are many services in the current namespace.
# If you experience slow pod startups you probably want to set this to `false`.
enableServiceLinks: true

protocol: https
httpPort: 9200
transportPort: 9300
metricsPort: 9600
httpHostPort: ""
transportHostPort: ""


service:
  labels: {}
  labelsHeadless: {}
  headless:
    annotations: {}
  type: ClusterIP
  # The IP family and IP families options are to set the behaviour in a dual-stack environment
  # Omitting these values will let the service fall back to whatever the CNI dictates the defaults
  # should be
  #
  # ipFamilyPolicy: SingleStack
  # ipFamilies:
  # - IPv4
  nodePort: ""
  annotations: {}
  httpPortName: http
  transportPortName: transport
  metricsPortName: metrics
  loadBalancerIP: ""
  loadBalancerSourceRanges: []
  externalTrafficPolicy: ""

updateStrategy: RollingUpdate

# This is the max unavailable setting for the pod disruption budget
# The default value of 1 will make sure that kubernetes won't allow more than 1
# of your pods to be unavailable during maintenance
maxUnavailable: 1

podSecurityContext:
  fsGroup: 1000
  runAsUser: 1000

securityContext:
  capabilities:
    drop:
      - ALL
  # readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000

securityConfig:
  enabled: true
  path: "/usr/share/lucenia/config/lucenia-security"
  actionGroupsSecret:
  configSecret:
  internalUsersSecret:
  rolesSecret:
  rolesMappingSecret:
  tenantsSecret:
  # The following option simplifies securityConfig by using a single secret and
  # specifying the config files as keys in the secret instead of creating
  # different secrets for for each config file.
  # Note that this is an alternative to the individual secret configuration
  # above and shouldn't be used if the above secrets are used.
  config:
    # There are multiple ways to define the configuration here:
    # * If you define anything under data, the chart will automatically create
    #   a secret and mount it. This is best option to choose if you want to override all the
    #   existing yml files at once.
    # * If you define securityConfigSecret, the chart will assume this secret is
    #   created externally and mount it. This is best option to choose if your intention is to
    #   only update a single yml file.
    # * It is an error to define both data and securityConfigSecret.
    securityConfigSecret: ""
    dataComplete: true
    data: {}
    #   config.yml: |-
    #   internal_users.yml: |-
    #   roles.yml: |-
    #   roles_mapping.yml: |-
    #   action_groups.yml: |-
    #   tenants.yml: |-

# Grace period to wait for Lucenia to stop gracefully
terminationGracePeriodSeconds: 120

startupProbe:
  tcpSocket:
    port: 9200
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 30

livenessProbe: {}
#   periodSeconds: 20
#   timeoutSeconds: 5
#   failureThreshold: 10
#   successThreshold: 1
#   initialDelaySeconds: 10
#   tcpSocket:
#     port: 9200

readinessProbe:
  tcpSocket:
    port: 9200
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

## Use an alternate scheduler.
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
schedulerName: ""

nodeSelector: {}
tolerations: []

# Enable ingress to expose Lucenia outside the kubernetes cluster
# Only enable this if security settings are properly configured
ingress:
  enabled: false
  # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
  # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
  # ingressClassName: nginx

  annotations: {}
  #   kubernetes.io/ingress.class: nginx
  #   kubernetes.io/tls-acme: "true"
  ingressLabels: {}
  path: /
  hosts:
    - chart-example.local
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

# Set to true to prevent slow manager node re-election
managerTerminationFix: false

luceniaLifecycle: {}
#   preStop:
#     exec:
#       command: ["/bin/sh", "-c", "echo Hello from the preStart handler > /usr/share/message"]
#   postStart:
#     exec:
#       command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

lifecycle: {}
# preStop:
#     exec:
#       command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
#   postStart:
#     exec:
#       command:
#         - bash
#         - -c
#         - |
#           #!/bin/bash
#           # Add a template to adjust number of shards/replicas1
#           TEMPLATE_NAME=my_template
#           INDEX_PATTERN="logstash-*"
#           SHARD_COUNT=8
#           REPLICA_COUNT=1
#           ES_URL=http://localhost:9200
#           while [[ "$(curl -s -o /dev/null -w '%{http_code}\n' $ES_URL)" != "200" ]]; do sleep 1; done
#           curl -XPUT "$ES_URL/_template/$TEMPLATE_NAME" -H 'Content-Type: application/json' -d'{"index_patterns":['\""$INDEX_PATTERN"\"'],"settings":{"number_of_shards":'$SHARD_COUNT',"number_of_replicas":'$REPLICA_COUNT'}}'

keystore: []
# To add secrets to the keystore:
#  - secretName: lucenia-encryption-key

networkPolicy:  # todo - confirm in testing
  create: false
  ## Enable creation of NetworkPolicy resources. Only Ingress traffic is filtered for now.
  ## In order for a Pod to access Lucenia, it needs to have the following label:
  ## {{ template "uname" . }}-client: "true"
  ## Example for default configuration to access HTTP port:
  ## lucenia-manager-http-client: "true"
  ## Example for default configuration to access transport port:
  ## lucenia-manager-transport-client: "true"

  http:
    enabled: false

## Set optimal sysctl's through securityContext. This requires privilege. Can be disabled if
## the system has already been preconfigured.
## https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/
sysctl:
  enabled: false

## Set optimal sysctl's through privileged initContainer.
sysctlInit:
  enabled: false
#   override image, which is busybox by default
#   image: busybox
#   override image tag, which is latest by default
#   imageTag:
# sysctlVmMaxMapCount to set through sysctlInit initContainer
sysctlVmMaxMapCount: 262144

ulimitInit:
  enabled: false
  # override image, which is busybox by default
  # image: busybox
  # override image tag, which is latest by default
  # imageTag:
# ulimitCount to set through ulimitInit initContainer
ulimitCount: 65536

## Enable to add 3rd Party / Custom plugins not offered in the default image.
plugins:
  enabled: false
  installList: []
#   - example-plugin

# -- Array of extra K8s manifests to deploy
extraObjects: []
#   - apiVersion: secrets-store.csi.x-k8s.io/v1
#     kind: SecretProviderClass
#     metadata:
#       name: argocd-secrets-store
#     spec:
#       provider: aws
#       parameters:
#         objects: |
#           - objectName: "argocd"
#             objectType: "secretsmanager"
#             jmesPath:
#                 - path: "client_id"
#                   objectAlias: "client_id"
#                 - path: "client_secret"
#                   objectAlias: "client_secret"
#       secretObjects:
#       - data:
#         - key: client_id
#           objectName: client_id
#         - key: client_secret
#           objectName: client_secret
#         secretName: argocd-secrets-store
#         type: Opaque
#         labels:
#           app.kubernetes.io/part-of: argocd
#   - |
#      apiVersion: policy/v1
#      kind: PodDisruptionBudget
#      metadata:
#        name: {{ template "lucenia.uname" . }}
#        labels:
#          {{- include "lucenia.labels" . | nindent 4 }}
#      spec:
#       minAvailable: 1
#        selector:
#          matchLabels:
#            {{- include "lucenia.selectorLabels" . | nindent 6 }}
